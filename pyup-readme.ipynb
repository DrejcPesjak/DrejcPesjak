{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./venv-drejc/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv-drejc/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv-drejc/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv-drejc/lib/python3.10/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv-drejc/lib/python3.10/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: huggingface_hub in ./venv-drejc/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv-drejc/lib/python3.10/site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv-drejc/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: filelock in ./venv-drejc/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv-drejc/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv-drejc/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: requests in ./venv-drejc/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv-drejc/lib/python3.10/site-packages (from huggingface_hub) (4.67.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv-drejc/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv-drejc/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv-drejc/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv-drejc/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: pillow in ./venv-drejc/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: feedparser in ./venv-drejc/lib/python3.10/site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in ./venv-drejc/lib/python3.10/site-packages (from feedparser) (1.0.0)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install huggingface_hub\n",
    "!pip install pillow\n",
    "!pip install feedparser\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "# Parse the RSS feed\n",
    "rss_url = \"https://buttondown.email/ainews/rss\"\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "# Extract titles and descriptions\n",
    "news_items = [\n",
    "    {\"title\": entry.title, \"link\": entry.link, \"description\": entry.description}\n",
    "    for entry in feed.entries\n",
    "]\n",
    "\n",
    "# Pick the most interesting article (e.g., the latest one)\n",
    "top_news = news_items[1] if news_items else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import html\n",
    "\n",
    "# # Get the HTML content\n",
    "# html_content = top_news[\"description\"]\n",
    "\n",
    "# # Parse and prettify the HTML\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "# pretty_html = soup.prettify()\n",
    "\n",
    "# # Display the prettified HTML\n",
    "# print(pretty_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h1 id=\"ai-reddit-recap\">AI Reddit Recap</h1>\\n<h2 id=\"rlocalllama-recap\">/r/LocalLlama Recap</h2>\\n<p><strong>Theme 1. Nvidia RTX 5090 enters production with 32GB VRAM</strong></p>\\n<ul>\\n<li><strong>Nvidia RTX 5090 with 32GB of RAM rumored to be entering production</strong> (<a href=\"https://reddit.com/r/LocalLLaMA/comments/1gqk300/nvidia_rtx_5090_with_32gb_of_ram_rumored_to_be/\" target=\"_blank\">Score: 271, Comments: 139</a>): <strong>Nvidia</strong> is reportedly shifting its production focus to the <strong>RTX 50 series</strong>, with the <strong>RTX 5090</strong> rumored to feature <strong>32GB of RAM</strong>. Concerns are rising about potential scalper activity affecting the availability and pricing of these new GPUs, as highlighted in multiple sources including <a href=\"https://videocardz.com/newz/nvidia-shifts-production-to-geforce-rtx-50-series-only-one-ada-gpu-reportedly-still-in-production\" target=\"_blank\">VideoCardz</a> and <a href=\"https://www.pcgamesn.com/nvidia/geforce-rtx-5000-soon\" target=\"_blank\">PCGamesN</a>.<ul>\\n<li>There is skepticism about the <strong>32GB RAM rumor</strong> for the <strong>RTX 5090</strong>, with some users questioning the validity of the sources and suggesting that <strong>Nvidia</strong> might change specifications last minute, referencing past incidents like the <strong>4080/4070 fiasco</strong>. The rumor of 32GB VRAM has been circulating widely, but it remains unconfirmed by official sources.</li>\\n<li>Users express concerns over <strong>scalper activity</strong> and high pricing, with predictions of prices reaching <strong>$3000</strong> or more due to scalpers and market demand. Some comments discuss the potential impact of Nvidia\\'s production shifts and legal restrictions, like the inability to sell in China, on the availability and pricing in other regions such as the European Union.</li>\\n<li>Discussions highlight the <strong>use cases of RTX 5090</strong> beyond gaming, focusing on professional and hobbyist applications like running local models and AI tasks. Users compare the potential performance and VRAM requirements of the 5090 with current models like the <strong>RTX 3090</strong> and emphasize the importance of VRAM in handling tasks like AI video generation and large language models.</li>\\n</ul>\\n</li>\\n</ul>\\n<p><strong>Theme 2. MMLU-Pro scores: Qwen and Claude Sonnet models</strong></p>\\n<ul>\\n<li><strong><a href=\"https://i.redd.it/e7fs0yxafq0e1.png\" target=\"_blank\">MMLU-Pro score vs inference costs</a></strong> (<a href=\"https://reddit.com/r/LocalLLaMA/comments/1gqna7c/mmlupro_score_vs_inference_costs/\" target=\"_blank\">Score: 215, Comments: 31</a>): <strong>MMLU-Pro score</strong> and <strong>inference costs</strong> are likely the focus of analysis, examining the relationship between model performance metrics and the financial implications of running inference tasks. This discussion is relevant for engineers optimizing AI models for cost-efficiency while maintaining high performance.<ul>\\n<li><strong>Claude Sonnet 3.5</strong> is praised for its versatility and accuracy in handling complex tasks, though it requires specific prompting for novel solutions. It is considered a highly efficient tool for programmers due to its ability to understand and solve errors quickly.</li>\\n<li>The <strong>Tencent Hunyuan model</strong> is noted for its high <strong>MMLU</strong> score and its architecture as a mixture of experts with <strong>52 billion active parameters</strong>. This model is suggested as potentially outperforming existing models like Sonnet 3.5.</li>\\n<li>Discussions highlight the <strong>Qwen models</strong> as cost-effective, with <strong>Qwen 2.5</strong> prominently defining the Pareto curve for performance and cost efficiency. The <strong>Haiku model</strong> is criticized for being overpriced, and the analysis of inference costs shows <strong>Claude 3.5 Sonnet</strong> has significantly higher costs compared to <strong>70B models</strong>.</li>\\n</ul>\\n</li>\\n</ul>\\n<p><strong>Theme 3. Qwen2.5 RPMax v1.3: Creative Writing Model</strong></p>\\n<ul>\\n<li><strong><a href=\"https://huggingface.co/ArliAI/Qwen2.5-32B-ArliAI-RPMax-v1.3\" target=\"_blank\">Write-up on repetition and creativity of LLM models and New Qwen2.5 32B based ArliAI RPMax v1.3 Model!</a></strong> (<a href=\"https://reddit.com/r/LocalLLaMA/comments/1gqo7f0/writeup_on_repetition_and_creativity_of_llm/\" target=\"_blank\">Score: 103, Comments: 60</a>): The post discusses the <strong>Qwen2.5 32B based ArliAI RPMax v1.3 Model</strong>, focusing on its <strong>repetition and creativity</strong> in the context of <strong>LLM (Large Language Model) performance</strong>. The absence of a detailed post body limits specific insights into the model\\'s training methods or performance metrics.<ul>\\n<li><strong>Model Versions and Training Improvements</strong>: The discussion highlights the evolution of the <strong>RPMax</strong> model from <strong>v1.0 to v1.3</strong>, with improvements in training parameters and dataset curation. Notably, <strong>v1.3</strong> uses <strong>rsLoRA+</strong> for better learning and lower loss, and the model is praised for its creativity and reduced repetition in writing tasks.</li>\\n<li><strong>Dataset and Fine-Tuning Strategy</strong>: The model\\'s success is attributed to a curated dataset that avoids repetition and focuses on quality over quantity. The training involves only a single epoch with a higher learning rate, aiming for creative output rather than exact replication of training data, which differs from traditional fine-tuning methods.</li>\\n<li><strong>Community Feedback and Model Performance</strong>: Users report that the model achieves its goal of being a creative writing/RP model, with some describing interactions as feeling almost like engaging with a real person. The model\\'s performance in creative writing is discussed, with comparisons to other models like <strong>EVA-Qwen2.5-32B</strong> for context handling and writing quality.</li>\\n</ul>\\n</li>\\n</ul>\\n<p><strong>Theme 4. Qwen 32B vs 72B-Ins on Leetcode Comparison</strong></p>\\n<ul>\\n<li><strong>Qwen 32B Coder-Ins vs 72B-Ins on the latest Leetcode problems</strong> (<a href=\"https://reddit.com/r/LocalLLaMA/comments/1gr35xp/qwen_32b_coderins_vs_72bins_on_the_latest/\" target=\"_blank\">Score: 79, Comments: 23</a>): The post evaluates the performance of <strong>Qwen 32B Coder</strong> versus <strong>72B non-coder variant</strong> and <strong>GPT-4o</strong> on recent <strong>Leetcode</strong> problems, highlighting the models\\' strengths in reasoning over pure coding. Tests were conducted using <strong>vLLM</strong> with models quantized to <strong>FP8</strong> and a <strong>32,768-token context length</strong>, running on <strong>H100 GPUs</strong>. The author notes that this benchmark is 70% reasoning and 30% coding, emphasizing that hard Leetcode problems were mostly excluded due to their complexity and the models\\' generally poor performance on them.<ul>\\n<li>The author confirms that all test results are based on <strong>pass@1</strong>, which is a common metric for evaluating model performance on coding tasks. A user suggests expanding the tests to include <strong>14B and 7B coders</strong> for broader comparison, and the author expresses openness to this if there is enough interest, potentially leading to an open-source project.</li>\\n<li>One commenter suggests that the skill required to solve Leetcode problems has become more accessible due to advancements in AI, equating the skillset to the size of a <strong>PS4 game</strong>. Another user counters that this raises the <strong>skill floor</strong>, implying that while AI can handle simpler tasks, more complex problem-solving skills are still necessary.</li>\\n<li>There is interest in comparing different quantization methods, specifically <strong>FP8</strong> versus <strong>Q4_K_M</strong>, to determine which is better for inference. This highlights ongoing curiosity about the efficiency and performance trade-offs in model quantization techniques.</li>\\n</ul>\\n</li>\\n</ul>\\n<p style=\"height: 16px; margin: 0px !important;\"></p>\\n<h2 id=\"other-ai-subreddit-recap\">Other AI Subreddit Recap</h2>\\n<blockquote>\\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT</p>\\n</blockquote>\\n<p><strong>Theme 1. Gemini 1.5 Pro Released - Claims Top Spot on LMSys Leaderboard</strong></p>\\n<ul>\\n<li><strong><a href=\"https://www.reddit.com/gallery/1gr7nxt\" target=\"_blank\">Gemini-1.5-Pro, the BEST vision model ever, WITHOUT EXCEPTION, based on my personal testing</a></strong> (<a href=\"https://reddit.com/r/OpenAI/comments/1gr7nxt/gemini15pro_the_best_vision_model_ever_without/\" target=\"_blank\">Score: 48, Comments: 28</a>): <strong>Gemini-1.5-Pro</strong> appears to be a multimodal vision model, but without any post content or testing details provided, no substantive claims about its performance can be verified. The title makes subjective claims about the model\\'s superiority but lacks supporting evidence or comparative analysis.<ul>\\n<li>Users noted varying performance across different tasks, with one reporting that for <strong>graph analysis</strong>, their testing showed <strong>Claude Sonnet 3.5</strong> > <strong>GPT-4</strong> > <strong>Gemini-1.5-Pro</strong>, though others cautioned against drawing conclusions from limited testing samples.</li>\\n<li>Discussion of <strong>multimodal capabilities</strong> highlighted both strengths and limitations, with users noting that while <strong>Gemini</strong> and <strong>Imagen</strong> are underrated for multimodal input and image generation, the technology isn\\'t yet advanced enough for real-time webcam interaction.</li>\\n<li>Specific image analysis comparisons showed mixed accuracy, with <strong>Flash</strong> correctly identifying certain details (pigtails) while <strong>Pro</strong> provided more comprehensive descriptions, though both had some inaccuracies in their observations.</li>\\n</ul>\\n</li>\\n<li><strong><a href=\"https://i.redd.it/bf5yaps3mw0e1.jpeg\" target=\"_blank\">New Gemini model #1 on lmsys leaderboard above o1 models ? Anthropic release 3.5 opus soon</a></strong> (<a href=\"https://reddit.com/r/ClaudeAI/comments/1gragw5/new_gemini_model_1_on_lmsys_leaderboard_above_o1/\" target=\"_blank\">Score: 163, Comments: 57</a>): <strong>Google\\'s Gemini</strong> has reached the #1 position on the <strong>LMSys leaderboard</strong>, surpassing <strong>OpenAI\\'s models</strong> in performance rankings. <strong>Anthropic</strong> plans to release their new <strong>Claude 3.5 Opus</strong> model in the near future.<ul>\\n<li><strong>LMSYS leaderboard</strong> is criticized for lacking quality control and being based solely on user votes about formatting rather than actual performance. Multiple users point to <strong>LiveBench</strong> as a more reliable benchmark for model evaluation.</li>\\n<li>Users debate the performance of <strong>Claude 3.5 Sonnet</strong> (also referred to as <strong>3.6</strong>), with some highlighting its <strong>32k input context</strong> and slower but more thorough \"thinking\" approach. Several alternative benchmarking resources were shared, including <a href=\"https://scale.com/leaderboard\" target=\"_blank\">Scale.com</a> and <a href=\"https://livebench.ai/\" target=\"_blank\">LiveBench.ai</a>.</li>\\n<li><strong>Anthropic\\'s CEO Dario</strong> acknowledged in a <strong>Lex interview</strong> that naming both versions \"3.5\" was confusing and suggested they should have called the new version \"3.6\" instead. The company has recently removed the \"new\" label from their UI for the model.</li>\\n</ul>\\n</li>\\n</ul>\\n<p><strong>Theme 2. Undetectable ML Model Backdoors Using Digital Signatures - New Research</strong></p>\\n<ul>\\n<li><strong>[R] Undetectable Backdoors in ML Models: Novel Techniques Using Digital Signatures and Random Features, with Implications for Adversarial Robustness</strong> (<a href=\"https://reddit.com/r/MachineLearning/comments/1gr4ksm/r_undetectable_backdoors_in_ml_models_novel/\" target=\"_blank\">Score: 27, Comments: 5</a>): The research demonstrates how to construct <strong>undetectable backdoors</strong> in ML models using two frameworks: <strong>digital signature scheme-based</strong> backdoors and <strong>Random Fourier Features/Random ReLU</strong> based backdoors, which remain undetectable even under <strong>white-box analysis</strong> and with full access to model architecture, parameters, and training data. The findings reveal critical implications for <strong>ML security</strong> and <strong>outsourced training</strong>, showing that backdoored models maintain identical generalization error as clean models while allowing arbitrary output manipulation through subtle input perturbations, as detailed in their paper <a href=\"https://arxiv.org/abs/2204.06974\" target=\"_blank\">\"Planting Undetectable Backdoors in Machine Learning Models\"</a>.</li>\\n</ul>\\n<p><strong>Theme 3. New CogVideoX-5B Open Source Text-to-Video Model Released</strong></p>\\n<ul>\\n<li><strong><a href=\"https://v.redd.it/p7zhifwq3t0e1\" target=\"_blank\">CogvideoX + DimensionX (Comfy Lora Orbit Left) + Super Mario Bros. [NES]</a></strong> (<a href=\"https://reddit.com/r/StableDiffusion/comments/1gqy8kl/cogvideox_dimensionx_comfy_lora_orbit_left_super/\" target=\"_blank\">Score: 52, Comments: 4</a>): A post referencing <strong>CogVideoX 5B</strong> and <strong>DimensionX</strong> models used with <strong>Super Mario Bros NES</strong> content, though no specific details or examples were provided in the post body. The combination suggests video generation capabilities using these AI models with retro gaming content.</li>\\n</ul>\\n<ul>\\n<li><strong>CogVideoX-5b multiresolution finetuning on 4090</strong> (<a href=\"https://reddit.com/r/StableDiffusion/comments/1gqzo94/cogvideox5b_multiresolution_finetuning_on_4090/\" target=\"_blank\">Score: 21, Comments: 0</a>): <strong>CogVideoX-5b</strong> model can be fine-tuned using <strong>LoRA</strong> on an <strong>NVIDIA RTX 4090</strong> GPU using the <a href=\"https://github.com/a-r-r-o-w/cogvideox-factory/\" target=\"_blank\">cogvideox-factory</a> repository. The post includes a video demonstration of the fine-tuning process.</li>\\n</ul>\\n<p><strong>Theme 4. StackOverflow Traffic Plummets as AI Tools Rise</strong></p>\\n<ul>\\n<li><strong><a href=\"https://i.redd.it/dimb0c06pv0e1.jpeg\" target=\"_blank\">RIP Stackoverflow</a></strong> (<a href=\"https://reddit.com/r/ChatGPT/comments/1gr66cr/rip_stackoverflow/\" target=\"_blank\">Score: 703, Comments: 125</a>): <strong>Stack Overflow</strong> experienced a significant <strong>traffic decline</strong> after the rise of <strong>AI coding tools</strong>, leading to discussions about the future viability of traditional programming Q&amp;A platforms. The lack of post body content prevents a more detailed analysis of specific metrics or causes of this decline.<ul>\\n<li>Users overwhelmingly criticize <strong>Stack Overflow\\'s</strong> toxic culture, with a <strong>40-year software engineering veteran</strong> receiving <strong>552 upvotes</strong> for condemning the platform\\'s arrogant attitude, and multiple users citing frustration with the \"<em>duplicate question</em>\" responses and dismissive treatment of newcomers.</li>\\n<li>Concerns about <strong>model collapse</strong> and <strong>AI training data</strong> were raised, as the decline in <strong>Stack Overflow</strong> traffic could lead to outdated information sources for future AI models, with users noting that AI tools still rely on human-annotated data for training.</li>\\n<li>Multiple developers express preference for <strong>ChatGPT\\'s</strong> friendlier approach to answering questions, with users highlighting that AI tools provide immediate responses without the gatekeeping and hostility experienced on <strong>Stack Overflow</strong>, particularly noting that <strong>GPT</strong> was released in late <strong>2022</strong>.</li>\\n</ul>\\n</li>\\n</ul>\\n<ul>\\n<li><strong><a href=\"https://i.redd.it/7vnwwf74ut0e1.png\" target=\"_blank\">ChatGPT doesn’t have a shitty attitude when you ask a relevant question either.</a></strong> (<a href=\"https://reddit.com/r/ChatGPT/comments/1gr09al/chatgpt_doesnt_have_a_shitty_attitude_when_you/\" target=\"_blank\">Score: 221, Comments: 25</a>): <strong>ChatGPT</strong> provides a more welcoming environment for asking technical questions compared to <strong>Stack Overflow\\'s</strong> known hostile community responses. The post implies that <strong>ChatGPT</strong> delivers answers without the negative attitudes sometimes encountered on <strong>Stack Overflow</strong> when users ask legitimate questions.<ul>\\n<li>Users strongly criticize <strong>Stack Overflow\\'s</strong> toxic culture, with multiple examples of questions being marked as duplicates linking to <strong>14-year-old obsolete answers</strong>. The community\\'s elitist behavior includes dismissive responses and hostile treatment of new users.</li>\\n<li><strong>ChatGPT</strong> learned from a broad range of internet content including <strong>public GitHub repositories</strong> and <strong>pastebin scripts</strong>, not just Stack Overflow. The AI provides a more approachable platform for asking repeated or basic questions without fear of negative feedback.</li>\\n<li>The post references a traffic bump in <strong>July 2023</strong> coinciding with the launch of <a href=\"https://stackoverflow.blog/2023/07/27/announcing-overflowai/\" target=\"_blank\">OverflowAI</a>. Users note that <strong>Stack Exchange</strong> forums beyond programming, such as physics and electrical engineering, suffer from similar toxicity issues.</li>\\n</ul>\\n</li>\\n</ul>\\n<hr />\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract description and remove Reddit Recap section\n",
    "desc = top_news[\"description\"]\n",
    "start_idx = desc.find(\"<h1 id=\\\"ai-reddit-recap\\\">\")\n",
    "end_idx = desc.find(\"<h1 id=\\\"ai-discord-recap\\\">\")\n",
    "if start_idx != -1 and end_idx != -1:\n",
    "    desc = desc[start_idx:end_idx]\n",
    "desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": f\"\"\"From the following list of news items, pick the most interesting few and do a very short but appealing summary of it.\n",
    "\t\t\t\t\t{desc}\"\"\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "# messages = [\n",
    "# \t{\n",
    "# \t\t\"role\": \"user\",\n",
    "# \t\t\"content\": f\"\"\"\n",
    "#         From the following list of news items, pick the most interesting one. \n",
    "#         Then, write a detailed, vivid description focusing on visual elements or themes that could inspire an AI image generation model.\n",
    "#         Write it out all in one paragraph, no headings.\n",
    "#         {desc}\n",
    "#         \"\"\"\n",
    "# \t}\n",
    "# ]\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": f\"\"\"\n",
    "#         From the following list of AI news items, pick the single most interesting one. Name title of the article.\n",
    "#         Then convert the news item into a prompt for an AI image generation model. Make it as vivid and detailed as possible.\n",
    "#         The description must be no more than three sentences and strictly tied to the news content. Do not make it general.\n",
    "#         Get creative with the prompt, but keep it focused on the 1 article.\n",
    "#         Note that the image generator is not good at writing text in images.\n",
    "\n",
    "#         AI News items:\n",
    "#         {desc}\n",
    "#         \"\"\"\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## **AI News: Gemini 1.5 Pro - New Vision Model Outperforms GPT-4 and OpenAI Models**\n",
      "\n",
      "**Main Point:** A new AI vision model, **Gemini 1.5 Pro**, has topped the prestigious **LMSys leaderboard**, surpassing models from both Google and OpenAI. \n",
      "\n",
      "**Key Highlights:**\n",
      "\n",
      "* The model exhibits remarkable capabilities in image generation and analysis, outperforming GPT-4 and other leading models in tests.\n",
      "* Despite the lack of formal benchmarks and limited user feedback, the model's performance suggests significant potential for diverse applications.\n",
      "* Concerns arise about the fairness of the LMSys leaderboard and the need for more reliable evaluation methods for AI models."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=HUGGINGFACE_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "        From the following list of AI news items, pick the single most interesting one and do an appealing catchy headline for it.\n",
    "\n",
    "        AI News items:\n",
    "        {desc}\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"google/gemma-1.1-7b-it\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500, # is for the output of the model\n",
    "\tstream=True,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Most Interesting AI News:\\n\\n**1. Nvidia RTX 5090 Entering Production with 32GB VRAM:** \\n- Speculation surrounding the release of the RTX 5090 GPU heightened.\\n- Rumor suggesting 32GB of RAM for this high-end graphics card raises concerns about potential scalping and pricing issues.\\n\\n\\n**2. Undetectable ML Model Backdoors Using Digital Signatures:**\\n- New research demonstrates the ability to create undetectable backdoors in ML models, raising concerns about the security and robustness of these models.\\n\\n\\n**3. New CogVideoX-5B Open Source Text-to-Video Model Released:**\\n- A new text-to-video model, CogVideoX-5b, has been released with improved multiresolution capabilities.\\n- The model can be fine-tuned using LoRA on an NVIDIA RTX 4090 GPU.\\n\\n\\n**4. Stack Overflow Traffic Plummets as AI Tools Rise:**\\n- The popularity of AI coding tools has allegedly led to a significant traffic decline on Stack Overflow.\\n- Concerns about the platform's hostile culture and outdated information are prominent among user discussions.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_output = \"\"\n",
    "for chunk in stream:\n",
    "    gemma_output += chunk.choices[0].delta.content\n",
    "gemma_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.content\n",
    "\n",
    "image_bytes = query({\n",
    "\t\"inputs\": gemma_output,\n",
    "})\n",
    "# You can access the image with PIL.Image for example\n",
    "import io\n",
    "from PIL import Image\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate filename with current datetime\n",
    "filename = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".png\"\n",
    "\n",
    "# Save to pictures folder\n",
    "image.save(f\"pictures/{filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_template = f\"\"\"\n",
    "# Today's AI News\n",
    "\n",
    "![Todays Image](pictures/{filename})\n",
    "\n",
    "{gemma_output}\n",
    "\"\"\"\n",
    "\n",
    "# Write the new content to README\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(readme_template)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-drejc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
