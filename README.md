
# Today's AI News

![Todays Image](pictures/20241228_100957.png)

## Interesting News from AI Subreddits:

**DeepSeek's Cost-Effective AI:**

- DeepSeek's new AI model, DeepSeek-V3, offers impressive performance at just 2% the cost of competitors like Claude.
- Trained on just 8-11% of the budget of other large models, DeepSeek achieved remarkable cost-efficiency through innovative architecture and parallel training.

**DeepSeek's Massive MoE Architecture:**

- DeepSeek-V3 is a 671 billion parameter Mixture-of-Experts (MoE) model, trained on 2048 H800 GPUs.
- This massive model demonstrates remarkable training and inference efficiency, even running efficiently on CPUs.

**OpenAI's Financial Challenges:**

- OpenAI faces potential funding struggles, requiring more capital than expected for its for-profit plan.
- Skepticism surrounds the validity of their estimated $7 trillion fundraising goal and the sustainability of their business model.

**AI and Humor: Capybara Conversation:**

- A humorous interaction with ChatGPT showcases the AI's ability to engage in playful conversations and witty exchanges.
- The discussion highlights the potential for playful and engaging AI interactions in future generations.

**AI and Math: Progress, But Limitations Exist:**

- While AI can tackle mathematical problems, comparisons to human mathematicians remain inconclusive.
- Concerns exist regarding misleading descriptions of mathematical competitions and accurately assessing AI's performance.
